#### Git Repos: 

  https://github.com/sandhyat/PeriOperative_RiskPrediction  

 https://github.com/sandhyat/Epic_ContrastiveLearning_all_modalities  

 

##### Repo (1) organization: 
This repo contains the code to run all methods and the corresponding hyper parameter tuning except for Multiview contrastive learning (MVCL). The Hp tuning of MVCL is available in repo (2) where it was originally developed. The purpose for each file of repo (1) is available in the readme. In terms of repo organization, the methods are arranged in directories based on whether they are implemented in an end-to-end manner or two stage manner. The latter refers to first learning a representation and then training a classifier on the learnt representation.  

Within end-to-end directory, there are methods that only use data that is available preoperatively (XGBT, TabNet) and the others that use preoperative+ intraoperative data (XGBT_tsSummary, LSTM, Transformer). Within two stage directory, we have Scarf method that uses contrastive learning to learn self-supervised representations of preoperative data and MVCL that learns self-supervised rep of the time series data using all the data modalities that are available at the end of the surgery. MVCL also uses alerts generated during the surgery and postoperative outcomes to learn the representation. Finally, XGBT classifier is learnt on the self-supervised representations. 

For the majority of methods, there are three files: 1) Main method file that is capable of running the best setting model, 2) HP tuning file that uses optuna, 3) bash file that submits HP tuning file for all ablations. Here ablations refer to the combination of different data modalities. 

The code was implemented in a docker environment with the input and outout volumes as follows. The docker image used can be accessed at ‘docker121720/pytorch-for-ts:1.25’. 

##### Input data: 
As of September 30, 2024, the input data version used for Epic era is v1.3.2. The hyperparameter tuned models and the best results are based on this data version. During initial testing, the MetaVision data version used is from v1.3.3. When learning a predictor on combined dataset, v1.3.2 is mapped to /input/ volume and v1.3.3 is mapped to /input1/ volume when running inside a docker container. This is how it is read in the code file TS_MV_training.py that is currently capable of reading the combined dataset from two eras and train a model on it. 

##### Output results: 
The output from the codes in repo (1) is available in ‘/storage1/fs1/christopherking/Active/sandhyat/Output-TS_docker_July2024/’ and further organized into two folders.  

1) **‘HP_output’** has all the hyperparameter tuning results.  Particularly, for XGBT, Tabnet, Scarf, LSTM/Transformer, we used Optuna (https://optuna.readthedocs.io/en/stable/) for hyperparameter tuning. It identifies a set of hyperparameters in a trial and runs multiple trials. The output here is saved in two files. One that enlists all the trial values along with the hyperparameters in a csv file. Another file that has the best hyperparameters along with the best value saved in a json file. The file name contains outcome name, method name, ablation name (the data modalities used), a random number and the date and time when the file was saved. The random number exists because for a fixed setting the Hp tuning is performed 5 times starting from 5 random numbers. The test set (top 20% in the Epic case) was not used during this process. 

2) **‘Best_results’** has the results for the best settings identified by HP tuning. The results in this folder are generated by adding the ‘--bestModel’ argument when running the main method file. This folder contains the preoperative and intraoperative folders further categorized into individual methods.  In ‘Best_results /Preoperative’ folder, for each outcome, there are three pickle files which summarize the results for all ablations and all preoperative methods for the given outcome. The ‘perf_metric’ file has the AUROC/AUPRC or R2/Corr/pvalue/MAE/MSE values for Epic and Wave 2 era as reflected in the name. The ‘pred_combined’ file has the test set labels and corresponding predictions for a given outcome across all methods and ablations. Similarly, in ‘Best_results/Intraoperative’ folder also consists of the three set of files for all outcomes. However, the ‘perf_metric’ files here for the wave2 only contain the results on ‘XGBT_tsSum’ method for all ablations. In ‘Best_results /Preoperative/XGBT/$ablation$’ there is a metadata file along with saved models. This structure is similar for all methods and outcomes.  

For MVCL method, the best model file is implemented in similar way. However, the HP tuning is performed as described in the repo (2). The HP tuning in this setting follows creating a sobol grid, submitting a job for each combination in the grid and later identifying the best setting from a csv that saved the sobol grid results. I could have made it more automated and used Optuna here too. The latest HP results for MVCL which includes the alert_type information too is available at ‘/storage1/fs1/christopherking/Active/sandhyat/Output-from-docker-CL-TS’.  The jobs that led to these results are available in repo (2) file  ‘bsub_command_TS_contra_$outcomename$_IncludesAlertType.txt’. In the ‘Output-from-docker-CL-TS/training/’ folder, identify the best setting based on the name and time, copy the metadata json and label_te pickle files to the ‘HP_output’ folder above. Additionally, from the Output-from-docker-CL-TS/TrainedModels/’ folder, copy the best pred file that contains the predictions and the combined perf metric for each outcome to the ‘HP_output’. This is being done so that we don’t have to run the best setting of the biggest (all data modality) ablation again. Instead, we use results for the biggest ablation from here and use the best metadata to run the best setting of other MVCL ablations. ‘MVCL_best_setting_run.bash’ in the two-stage code directory takes care of this part for an outcome that is supplied as an argument to this bash file. 

**Compilation of all results**: `‘Result_compiler.ipynb’` notebook in repo (1) reads the best result pickle files from all the settings and creates tables arranged by outcomes, ablations and methods for preoperative and intraoperative methods separately. It also generates latex code for these tables which can be directly pasted (could have done better here) in latex file. It performs the above action for both Epic (test set) and wave2 era data.    

##### Job submitting commands: 

**All except MVCL**: 

1) HP tuning: There are ‘.sh’ files with docker command as comments in the file that will run the HP tuning tasks.  

2) Best setting: ‘best_setting_example.sh’ contains examples for preoperative and intraoperative files that run the best settings and saves the results for various outcomes. 

**MVCL**: 

1) In repo (2), ‘Bsub_launcher_CL_TS_HP.py’ can launch the hyperparameter tuning jobs for MVCL. It needs the outcome as an argument. 

2) Best setting: In repo (1), ‘Two_stage_selfsupervised/MVCL_best_setting_run.sh’ can be used to run the best MVCL settings for an outcome that is passed as an argument. 

##### Notes general: 

1) When missingness mask is active, at least 256G is needed on RIS. Otherwise 128G is sufficient even in MVCL case where all data modalities are used. 

2) Flowsheet imputation preprocessing: In repo (1), ‘End_to_end_supervised/Flowsheet_imputation.py’ contains code to generate the ‘Imputed_other_flow.feather’ and ‘Imputed_very_dense_flow.feather’ files. These feather files are needed before implementing any of the time series methods that rely on flowsheet data. During this process, ‘Xgboost_model_Other_flow’ folder containing the xgbt model for each flowsheet (measure index) is saved in the input directory. This is repeated for every era. 